{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6204beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\araut1\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Acc: 0.4667 | AUC: 0.4107\n",
      "Val Acc: 0.8000 | AUC: 0.7500\n",
      "Epoch 2/10\n",
      "Train Acc: 0.5333 | AUC: 0.4821\n",
      "Val Acc: 0.8000 | AUC: 0.6875\n",
      "Epoch 2/10\n",
      "Train Acc: 0.5333 | AUC: 0.4821\n",
      "Val Acc: 0.8000 | AUC: 0.6875\n",
      "Epoch 3/10\n",
      "Train Acc: 0.5333 | AUC: 0.5714\n",
      "Val Acc: 0.8000 | AUC: 0.5625\n",
      "Epoch 3/10\n",
      "Train Acc: 0.5333 | AUC: 0.5714\n",
      "Val Acc: 0.8000 | AUC: 0.5625\n",
      "Epoch 4/10\n",
      "Train Acc: 0.5333 | AUC: 0.6250\n",
      "Val Acc: 0.2000 | AUC: 0.4375\n",
      "Epoch 4/10\n",
      "Train Acc: 0.5333 | AUC: 0.6250\n",
      "Val Acc: 0.2000 | AUC: 0.4375\n",
      "Epoch 5/10\n",
      "Train Acc: 0.4667 | AUC: 0.6607\n",
      "Val Acc: 0.2000 | AUC: 0.3125\n",
      "Epoch 5/10\n",
      "Train Acc: 0.4667 | AUC: 0.6607\n",
      "Val Acc: 0.2000 | AUC: 0.3125\n",
      "Epoch 6/10\n",
      "Train Acc: 0.4667 | AUC: 0.6964\n",
      "Val Acc: 0.2000 | AUC: 0.3750\n",
      "Epoch 6/10\n",
      "Train Acc: 0.4667 | AUC: 0.6964\n",
      "Val Acc: 0.2000 | AUC: 0.3750\n",
      "Epoch 7/10\n",
      "Train Acc: 0.4667 | AUC: 0.7321\n",
      "Val Acc: 0.2000 | AUC: 0.3750\n",
      "Epoch 7/10\n",
      "Train Acc: 0.4667 | AUC: 0.7321\n",
      "Val Acc: 0.2000 | AUC: 0.3750\n",
      "Epoch 8/10\n",
      "Train Acc: 0.4667 | AUC: 0.7143\n",
      "Val Acc: 0.8000 | AUC: 0.3750\n",
      "Epoch 8/10\n",
      "Train Acc: 0.4667 | AUC: 0.7143\n",
      "Val Acc: 0.8000 | AUC: 0.3750\n",
      "Epoch 9/10\n",
      "Train Acc: 0.6000 | AUC: 0.7500\n",
      "Val Acc: 0.8000 | AUC: 0.3750\n",
      "Epoch 9/10\n",
      "Train Acc: 0.6000 | AUC: 0.7500\n",
      "Val Acc: 0.8000 | AUC: 0.3750\n",
      "Epoch 10/10\n",
      "Train Acc: 0.5333 | AUC: 0.7143\n",
      "Val Acc: 0.8000 | AUC: 0.3125\n",
      "Best Val AUC: 0.7500\n",
      "Epoch 10/10\n",
      "Train Acc: 0.5333 | AUC: 0.7143\n",
      "Val Acc: 0.8000 | AUC: 0.3125\n",
      "Best Val AUC: 0.7500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "# Configurations\n",
    "class Config:\n",
    "    img_size = 224\n",
    "    num_frames = 8\n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    lr = 3e-5\n",
    "    num_classes = 1\n",
    "    dataset_path = \"./\"\n",
    "    real_folder = \"real\"\n",
    "    fake_folder = \"fake\"\n",
    "\n",
    "# Simple face crop (Celeb-DF faces are centered)\n",
    "def center_crop(image):\n",
    "    h, w = image.shape[:2]\n",
    "    size = min(h, w)\n",
    "    y, x = (h - size) // 2, (w - size) // 2\n",
    "    return image[y:y+size, x:x+size]\n",
    "\n",
    "# Dataset\n",
    "class CelebDFDataset(Dataset):\n",
    "    def __init__(self, config, mode='train'):\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        self.samples = []\n",
    "        \n",
    "        # Load real and fake video paths\n",
    "        real_videos = [os.path.join(config.real_folder, f) \n",
    "                      for f in os.listdir(config.real_folder) if f.endswith('.mp4')]\n",
    "        fake_videos = [os.path.join(config.fake_folder, f) \n",
    "                      for f in os.listdir(config.fake_folder) if f.endswith('.mp4')]\n",
    "        \n",
    "        # Split (80/20)\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(real_videos)\n",
    "        split = int(0.8 * len(real_videos))\n",
    "        \n",
    "        real_videos = real_videos[:split] if mode == 'train' else real_videos[split:]\n",
    "        \n",
    "        # Create samples (0=real, 1=fake)\n",
    "        for vid in real_videos:\n",
    "            self.samples.append((vid, 0))\n",
    "        for vid in fake_videos:\n",
    "            self.samples.append((vid, 1))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_path, label = self.samples[idx]\n",
    "        frames = self._sample_frames(vid_path)\n",
    "        frames = torch.stack([transforms.ToTensor()(frame) for frame in frames])\n",
    "        return frames, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "    def _sample_frames(self, vid_path):\n",
    "        cap = cv2.VideoCapture(vid_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        indices = np.linspace(0, total_frames-1, self.config.num_frames, dtype=int)\n",
    "        \n",
    "        frames = []\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = center_crop(frame)\n",
    "                frame = Image.fromarray(frame).resize((self.config.img_size, self.config.img_size))\n",
    "                frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        # Ensure we have exactly num_frames frames\n",
    "        while len(frames) < self.config.num_frames:\n",
    "            if len(frames) > 0:\n",
    "                frames.append(frames[-1])  # Duplicate the last frame\n",
    "            else:\n",
    "                # If no frames were loaded, pad with black frames\n",
    "                black_frame = Image.new(\"RGB\", (self.config.img_size, self.config.img_size))\n",
    "                frames.append(black_frame)\n",
    "        return frames[:self.config.num_frames]  # Trim if we got extra frames\n",
    "\n",
    "# Model (using Vision Transformer)\n",
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Use pretrained ViT\n",
    "        vit_config = ViTConfig(\n",
    "            image_size=config.img_size,\n",
    "            num_hidden_layers=4,\n",
    "            num_attention_heads=4,\n",
    "            hidden_size=192\n",
    "        )\n",
    "        self.vit = ViTModel(vit_config)\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_attn = nn.MultiheadAttention(192, 4)\n",
    "        self.classifier = nn.Linear(192, config.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, frames, C, H, W)\n",
    "        batch, frames = x.shape[0], x.shape[1]\n",
    "        \n",
    "        # Process each frame with ViT\n",
    "        frame_features = []\n",
    "        for t in range(frames):\n",
    "            out = self.vit(x[:, t]).last_hidden_state[:, 0]  # CLS token\n",
    "            frame_features.append(out)\n",
    "        \n",
    "        # Temporal attention\n",
    "        features = torch.stack(frame_features, dim=1)  # (batch, frames, dim)\n",
    "        features = features.transpose(0, 1)  # (frames, batch, dim)\n",
    "        attn_out, _ = self.temporal_attn(features, features, features)\n",
    "        features = attn_out.mean(dim=0)  # (batch, dim)\n",
    "        \n",
    "        return torch.sigmoid(self.classifier(features)).squeeze()\n",
    "\n",
    "# Training\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    preds, truths = [], []\n",
    "    \n",
    "    for frames, labels in loader:\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds.extend(outputs.detach().cpu().numpy())\n",
    "        truths.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(truths, np.round(preds))\n",
    "    auc = roc_auc_score(truths, preds)\n",
    "    return acc, auc\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, labels in loader:\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "            outputs = model(frames)\n",
    "            \n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            truths.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(truths, np.round(preds))\n",
    "    auc = roc_auc_score(truths, preds)\n",
    "    return acc, auc\n",
    "\n",
    "def main():\n",
    "    config = Config()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Data\n",
    "    train_set = CelebDFDataset(config, 'train')\n",
    "    val_set = CelebDFDataset(config, 'val')\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=config.batch_size)\n",
    "    \n",
    "    # Model\n",
    "    model = DeepfakeDetector(config).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    \n",
    "    # Training loop\n",
    "    best_auc = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        train_acc, train_auc = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_acc, val_auc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "        print(f\"Train Acc: {train_acc:.4f} | AUC: {train_auc:.4f}\")\n",
    "        print(f\"Val Acc: {val_acc:.4f} | AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    \n",
    "    print(f\"Best Val AUC: {best_auc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fece443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b6f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678723b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75b85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
