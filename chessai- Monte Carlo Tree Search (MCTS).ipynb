{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Reinforcement Learning (AlphaZero-like Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch numpy chess tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import chess\n",
    "import chess.pgn\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from collections import deque, namedtuple\n",
    "import pickle\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chess Environment and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessGame:\n",
    "    \"\"\"Wrapper around python-chess for our needs\"\"\"\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board.reset()\n",
    "        return self.board\n",
    "    \n",
    "    def step(self, move):\n",
    "        \"\"\"Execute a move and return (new_state, reward, done, info)\"\"\"\n",
    "        if isinstance(move, str):\n",
    "            move = chess.Move.from_uci(move)\n",
    "        \n",
    "        self.board.push(move)\n",
    "        \n",
    "        # Check game status\n",
    "        done = self.board.is_game_over()\n",
    "        reward = 0\n",
    "        \n",
    "        if done:\n",
    "            result = self.board.result()\n",
    "            if result == \"1-0\":\n",
    "                reward = 1  # White wins\n",
    "            elif result == \"0-1\":\n",
    "                reward = -1  # Black wins\n",
    "            # else reward remains 0 for draw\n",
    "        \n",
    "        return self.board, reward, done, {}\n",
    "    \n",
    "    def legal_moves(self):\n",
    "        return list(self.board.legal_moves)\n",
    "    \n",
    "    def to_fen(self):\n",
    "        return self.board.fen()\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        return self.board.is_game_over()\n",
    "    \n",
    "    def current_player(self):\n",
    "        return self.board.turn\n",
    "    \n",
    "    def copy(self):\n",
    "        new_game = ChessGame()\n",
    "        new_game.board = self.board.copy()\n",
    "        return new_game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessNet(nn.Module):\n",
    "    \"\"\"Neural network that takes board position as input and outputs\n",
    "    policy (move probabilities) and value (expected outcome)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ChessNet, self).__init__()\n",
    "        \n",
    "        # Input: 8x8 board with 14 planes (6 piece types + colors, plus some meta info)\n",
    "        self.conv1 = nn.Conv2d(14, 256, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Policy head\n",
    "        self.policy_conv = nn.Conv2d(256, 2, kernel_size=1)\n",
    "        self.policy_fc = nn.Linear(2*8*8, 4672)  # 4672 is max possible moves in chess\n",
    "        \n",
    "        # Value head\n",
    "        self.value_conv = nn.Conv2d(256, 1, kernel_size=1)\n",
    "        self.value_fc1 = nn.Linear(8*8, 256)\n",
    "        self.value_fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Common trunk\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        # Policy head\n",
    "        p = F.relu(self.policy_conv(x))\n",
    "        p = p.view(-1, 2*8*8)\n",
    "        p = self.policy_fc(p)\n",
    "        \n",
    "        # Value head\n",
    "        v = F.relu(self.value_conv(x))\n",
    "        v = v.view(-1, 8*8)\n",
    "        v = F.relu(self.value_fc1(v))\n",
    "        v = torch.tanh(self.value_fc2(v))  # Output between -1 and 1\n",
    "        \n",
    "        return p, v\n",
    "\n",
    "def board_to_tensor(board):\n",
    "    \"\"\"Convert a chess.Board to a tensor representation\"\"\"\n",
    "    # Create a 14x8x8 tensor (6 piece types * 2 colors + some meta info)\n",
    "    tensor = torch.zeros(14, 8, 8, device=device)\n",
    "    \n",
    "    # Piece positions\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            # Piece type: pawn=0, knight=1, bishop=2, rook=3, queen=4, king=5\n",
    "            piece_type = piece.piece_type - 1\n",
    "            color = 0 if piece.color == chess.WHITE else 6\n",
    "            plane = color + piece_type\n",
    "            row, col = divmod(square, 8)\n",
    "            tensor[plane, row, col] = 1\n",
    "    \n",
    "    # Additional planes\n",
    "    # Plane 12: color to move (0 for black, 1 for white)\n",
    "    tensor[12] = 1 if board.turn == chess.WHITE else 0\n",
    "    \n",
    "    # Plane 13: total move count (normalized)\n",
    "    tensor[13] = board.fullmove_number / 100.0\n",
    "    \n",
    "    return tensor.unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Node in the Monte Carlo Tree\"\"\"\n",
    "    def __init__(self, game_state, parent=None, move=None):\n",
    "        self.game_state = game_state  # ChessGame instance\n",
    "        self.parent = parent\n",
    "        self.move = move  # Move that led to this node\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        self.prior = 0\n",
    "    \n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"Monte Carlo Tree Search implementation\"\"\"\n",
    "    def __init__(self, model, num_simulations=800, c_puct=1.0):\n",
    "        self.model = model\n",
    "        self.num_simulations = num_simulations\n",
    "        self.c_puct = c_puct\n",
    "    \n",
    "    def search(self, root_state):\n",
    "        \"\"\"Perform MCTS and return action probabilities\"\"\"\n",
    "        root = Node(root_state)\n",
    "        \n",
    "        for _ in range(self.num_simulations):\n",
    "            node = root\n",
    "            search_path = [node]\n",
    "            \n",
    "            # Selection\n",
    "            while node.expanded():\n",
    "                node = self.select_child(node)\n",
    "                search_path.append(node)\n",
    "            \n",
    "            # Expansion\n",
    "            parent = search_path[-1]\n",
    "            if not parent.game_state.is_game_over():\n",
    "                # Get policy and value from neural net\n",
    "                board_tensor = board_to_tensor(parent.game_state.board)\n",
    "                policy_logits, value = self.model(board_tensor)\n",
    "                policy_logits = policy_logits.squeeze(0).detach().cpu().numpy()\n",
    "                value = value.item()\n",
    "                \n",
    "                # Mask illegal moves\n",
    "                legal_moves = parent.game_state.legal_moves()\n",
    "                move_indices = [move_to_index(move) for move in legal_moves]\n",
    "                mask = torch.zeros(4672, device=device)\n",
    "                for idx in move_indices:\n",
    "                    if idx < 4672:  # Safety check\n",
    "                        mask[idx] = 1\n",
    "                \n",
    "                # Apply mask and softmax to get probabilities\n",
    "                policy_logits = torch.from_numpy(policy_logits).to(device)\n",
    "                policy_logits = policy_logits - 1000*(1 - mask)  # Large negative for illegal moves\n",
    "                policy = F.softmax(policy_logits, dim=0).cpu().numpy()\n",
    "                \n",
    "                # Expand node\n",
    "                for move in legal_moves:\n",
    "                    child_state = parent.game_state.copy()\n",
    "                    child_state.step(move)\n",
    "                    move_idx = move_to_index(move)\n",
    "                    child_node = Node(child_state, parent, move)\n",
    "                    child_node.prior = policy[move_idx]\n",
    "                    parent.children.append(child_node)\n",
    "            else:\n",
    "                # Game is over, get actual value\n",
    "                result = parent.game_state.board.result()\n",
    "                if result == \"1-0\":\n",
    "                    value = 1\n",
    "                elif result == \"0-1\":\n",
    "                    value = -1\n",
    "                else:\n",
    "                    value = 0\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.backpropagate(search_path, value)\n",
    "        \n",
    "        # Return visit counts as action probabilities\n",
    "        visit_counts = np.array([child.visit_count for child in root.children])\n",
    "        action_probs = visit_counts / np.sum(visit_counts)\n",
    "        \n",
    "        return action_probs, root\n",
    "    \n",
    "    def select_child(self, node):\n",
    "        \"\"\"Select child with highest UCB score\"\"\"\n",
    "        total_visits = sum(child.visit_count for child in node.children)\n",
    "        log_total_visits = math.log(total_visits) if total_visits > 0 else 0\n",
    "        \n",
    "        best_score = -float(\"inf\")\n",
    "        best_child = None\n",
    "        \n",
    "        for child in node.children:\n",
    "            # UCB score\n",
    "            exploit = child.value()\n",
    "            explore = self.c_puct * child.prior * math.sqrt(log_total_visits) / (child.visit_count + 1)\n",
    "            score = exploit + explore\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_child = child\n",
    "        \n",
    "        return best_child\n",
    "    \n",
    "    def backpropagate(self, search_path, value):\n",
    "        \"\"\"Backpropagate value through the search path\"\"\"\n",
    "        for node in reversed(search_path):\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            value = -value  # Alternate perspective for opponent\n",
    "\n",
    "def move_to_index(move):\n",
    "    \"\"\"Convert a chess move to an index in the policy output\"\"\"\n",
    "    # This is a simplified version - in practice you'd need a more comprehensive mapping\n",
    "    from_square = move.from_square\n",
    "    to_square = move.to_square\n",
    "    promotion = move.promotion or 0\n",
    "    \n",
    "    # Simple hash (this should be replaced with a proper mapping)\n",
    "    return from_square * 64 + to_square + promotion * 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Play and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayDataset(Dataset):\n",
    "    \"\"\"Dataset to store self-play games\"\"\"\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "    \n",
    "    def add_game(self, game_data):\n",
    "        self.data.extend(game_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        board_tensor, policy, value = self.data[idx]\n",
    "        return board_tensor, (policy, value)\n",
    "\n",
    "def self_play(model, num_games=100, num_simulations=100):\n",
    "    \"\"\"Generate self-play games\"\"\"\n",
    "    dataset = SelfPlayDataset()\n",
    "    mcts = MCTS(model, num_simulations=num_simulations)\n",
    "    \n",
    "    for _ in tqdm(range(num_games), desc=\"Self-play\"):\n",
    "        game = ChessGame()\n",
    "        game_data = []\n",
    "        \n",
    "        while not game.is_game_over():\n",
    "            # Get action probabilities from MCTS\n",
    "            action_probs, root = mcts.search(game)\n",
    "            \n",
    "            # Store training data\n",
    "            board_tensor = board_to_tensor(game.board)\n",
    "            policy = np.zeros(4672)\n",
    "            for child, prob in zip(root.children, action_probs):\n",
    "                move_idx = move_to_index(child.move)\n",
    "                policy[move_idx] = prob\n",
    "            \n",
    "            # Value is from the perspective of the current player\n",
    "            value = 0  # Will be updated when game ends\n",
    "            game_data.append((board_tensor, policy, value))\n",
    "            \n",
    "            # Choose move (with some temperature for exploration)\n",
    "            move = np.random.choice([child.move for child in root.children], p=action_probs)\n",
    "            game.step(move)\n",
    "        \n",
    "        # Update values based on game outcome\n",
    "        result = game.board.result()\n",
    "        if result == \"1-0\":\n",
    "            final_value = 1\n",
    "        elif result == \"0-1\":\n",
    "            final_value = -1\n",
    "        else:\n",
    "            final_value = 0\n",
    "        \n",
    "        # Assign values to each position (from perspective of player to move)\n",
    "        for i, (board_tensor, policy, _) in enumerate(game_data):\n",
    "            # Alternate perspective based on player turn\n",
    "            value = final_value if i % 2 == 0 else -final_value\n",
    "            game_data[i] = (board_tensor, policy, value)\n",
    "        \n",
    "        dataset.add_game(game_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_model(model, dataset, epochs=100, batch_size=32):\n",
    "    \"\"\"Train the model on self-play data\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        policy_loss_total = 0\n",
    "        value_loss_total = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            board_tensors, (policies, values) = batch\n",
    "            board_tensors = board_tensors.to(device).float()\n",
    "            policies = policies.to(device).float()\n",
    "            values = values.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            policy_pred, value_pred = model(board_tensors)\n",
    "            \n",
    "            # Losses\n",
    "            policy_loss = F.cross_entropy(policy_pred, policies)\n",
    "            value_loss = F.mse_loss(value_pred, values)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            policy_loss_total += policy_loss.item()\n",
    "            value_loss_total += value_loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_policy_loss = policy_loss_total / len(dataloader)\n",
    "        avg_value_loss = value_loss_total / len(dataloader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f} (Policy={avg_policy_loss:.4f}, Value={avg_value_loss:.4f})\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_chess_ai(num_iterations=100, num_self_play_games=20, num_epochs=5, num_simulations=100):\n",
    "    \"\"\"Main training loop\"\"\"\n",
    "    model = ChessNet().to(device)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\n=== Iteration {iteration+1}/{num_iterations} ===\")\n",
    "        \n",
    "        # Self-play\n",
    "        print(\"Generating self-play games...\")\n",
    "        dataset = self_play(model, num_games=num_self_play_games, num_simulations=num_simulations)\n",
    "        \n",
    "        # Training\n",
    "        print(\"Training model...\")\n",
    "        model = train_model(model, dataset, epochs=num_epochs)\n",
    "        \n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), f\"chess_ai_iteration_{iteration}.pth\")\n",
    "        print(f\"Model saved to chess_ai_iteration_{iteration}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and ELO Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, opponent, num_games=100, num_simulations=100):\n",
    "    \"\"\"Evaluate model against an opponent\"\"\"\n",
    "    mcts = MCTS(model, num_simulations=num_simulations)\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    \n",
    "    for game_num in tqdm(range(num_games), desc=\"Evaluation\"):\n",
    "        game = ChessGame()\n",
    "        \n",
    "        # Alternate who plays white\n",
    "        if game_num % 2 == 0:\n",
    "            players = [model, opponent]  # Model is white\n",
    "        else:\n",
    "            players = [opponent, model]  # Model is black\n",
    "        \n",
    "        while not game.is_game_over():\n",
    "            current_player = 0 if game.current_player() == chess.WHITE else 1\n",
    "            player = players[current_player]\n",
    "            \n",
    "            if player == model:\n",
    "                # Model uses MCTS\n",
    "                action_probs, _ = mcts.search(game)\n",
    "                legal_moves = game.legal_moves()\n",
    "                move = np.random.choice(legal_moves, p=action_probs)\n",
    "            else:\n",
    "                # Opponent makes a move (could be random, stockfish, etc.)\n",
    "                if opponent == \"random\":\n",
    "                    legal_moves = game.legal_moves()\n",
    "                    move = random.choice(legal_moves)\n",
    "                else:\n",
    "                    # For more sophisticated opponents\n",
    "                    move = opponent.make_move(game)\n",
    "            \n",
    "            game.step(move)\n",
    "        \n",
    "        # Record result\n",
    "        result = game.board.result()\n",
    "        if result == \"1-0\":\n",
    "            if game_num % 2 == 0:\n",
    "                wins += 1  # Model was white and won\n",
    "            else:\n",
    "                losses += 1  # Model was black and opponent won\n",
    "        elif result == \"0-1\":\n",
    "            if game_num % 2 == 0:\n",
    "                losses += 1  # Model was white and lost\n",
    "            else:\n",
    "                wins += 1  # Model was black and won\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    win_rate = wins / num_games\n",
    "    loss_rate = losses / num_games\n",
    "    draw_rate = draws / num_games\n",
    "    \n",
    "    print(f\"Results against opponent: Wins={wins}, Losses={losses}, Draws={draws}\")\n",
    "    print(f\"Win rate: {win_rate:.2%}, Loss rate: {loss_rate:.2%}, Draw rate: {draw_rate:.2%}\")\n",
    "    \n",
    "    return wins, losses, draws\n",
    "\n",
    "def estimate_elo(wins, losses, draws, opponent_elo=1500):\n",
    "    \"\"\"Estimate ELO rating based on performance against an opponent\"\"\"\n",
    "    total_games = wins + losses + draws\n",
    "    win_prob = (wins + 0.5 * draws) / total_games\n",
    "    \n",
    "    if win_prob == 1:\n",
    "        win_prob = 0.999  # Avoid division by zero\n",
    "    elif win_prob == 0:\n",
    "        win_prob = 0.001\n",
    "    \n",
    "    elo_difference = -400 * math.log10(1/win_prob - 1)\n",
    "    estimated_elo = opponent_elo + elo_difference\n",
    "    \n",
    "    return estimated_elo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    model = train_chess_ai(\n",
    "        num_iterations=1,  # For demo purposes - increase for better results\n",
    "        num_self_play_games=1,  # For demo purposes - increase for better results\n",
    "        num_epochs=5,  # For demo purposes - increase for better results\n",
    "        num_simulations=2  # For demo purposes - increase for better results\n",
    "    )\n",
    "    \n",
    "    # Evaluate against random opponent\n",
    "    print(\"\\nEvaluating against random opponent...\")\n",
    "    wins, losses, draws = evaluate_model(model, opponent=\"random\", num_games=10)\n",
    "    \n",
    "    # Estimate ELO (assuming random opponent is ~800 ELO)\n",
    "    elo = estimate_elo(wins, losses, draws, opponent_elo=800)\n",
    "    print(f\"Estimated ELO rating: {elo:.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
