{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260789a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Step 1: Preprocess the text data\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, text, seq_length=10):\n",
    "        self.text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        self.seq_length = seq_length\n",
    "        self.words = self.text.split()\n",
    "        self.word_counts = Counter(self.words)\n",
    "        self.vocab = sorted(self.word_counts, key=self.word_counts.get, reverse=True)\n",
    "        self.vocab_to_int = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.int_to_vocab = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.n_vocab = len(self.vocab)\n",
    "        \n",
    "    def create_sequences(self):\n",
    "        sequences = []\n",
    "        for i in range(len(self.words) - self.seq_length):\n",
    "            seq = self.words[i:i + self.seq_length]\n",
    "            next_word = self.words[i + self.seq_length]\n",
    "            sequences.append(([self.vocab_to_int[word] for word in seq], \n",
    "                            self.vocab_to_int[next_word]))\n",
    "        return sequences\n",
    "\n",
    "# Step 2: Create Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.sequences[idx]\n",
    "        return torch.tensor(sequence), torch.tensor([target])\n",
    "\n",
    "# Step 3: Define LSTM Model\n",
    "class NextWordLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, n_layers=2):\n",
    "        super(NextWordLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # Embed the input\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Forward pass through LSTM\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # Reshape output and pass through final layer\n",
    "        out = out.contiguous().view(-1, out.shape[2])\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_().to(device),\n",
    "                  weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_().to(device))\n",
    "        return hidden\n",
    "\n",
    "# Step 4: Training Function\n",
    "def train_model(model, dataloader, epochs=10, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        hidden = model.init_hidden(batch_size, device)\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            hidden = tuple([h.data for h in hidden])\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            \n",
    "            # Calculate loss and backpropagate\n",
    "            loss = criterion(outputs.view(inputs.size(0), -1, vocab_size)[:, -1, :], targets.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Step 5: Prediction Function\n",
    "def predict_next_word(model, preprocessor, initial_text, device, top_k=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Process input text\n",
    "    initial_text = initial_text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    words = initial_text.split()[-preprocessor.seq_length:]\n",
    "    \n",
    "    # Convert words to integers\n",
    "    seq = [preprocessor.vocab_to_int.get(word, 0) for word in words]\n",
    "    seq = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    with torch.no_grad():\n",
    "        outputs, _ = model(seq, hidden)\n",
    "        outputs = outputs[-1]  # Get the last output (next word prediction)\n",
    "        \n",
    "    # Get top predictions\n",
    "    probs = torch.softmax(outputs, dim=0)\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    # Convert to words and probabilities\n",
    "    top_words = [preprocessor.int_to_vocab[idx.item()] for idx in top_indices]\n",
    "    top_probs = top_probs.cpu().numpy()\n",
    "    \n",
    "    return list(zip(top_words, top_probs))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample text (in practice, use a larger corpus)\n",
    "    text = \"\"\"The quick brown fox jumps over the lazy dog. \n",
    "              The dog barked at the fox, but the fox kept running. \n",
    "              A quick movement caught the dog's attention.\"\"\"\n",
    "    \n",
    "    # Preprocess text\n",
    "    seq_length = 5\n",
    "    preprocessor = TextPreprocessor(text, seq_length)\n",
    "    sequences = preprocessor.create_sequences()\n",
    "    \n",
    "    # Create dataloader\n",
    "    batch_size = 2\n",
    "    dataset = TextDataset(sequences)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    vocab_size = preprocessor.n_vocab\n",
    "    model = NextWordLSTM(vocab_size)\n",
    "    \n",
    "    # Train model\n",
    "    train_model(model, dataloader, epochs=30)\n",
    "    \n",
    "    # Make predictions\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_text = \"the quick brown fox\"\n",
    "    predictions = predict_next_word(model, preprocessor, test_text, device)\n",
    "    \n",
    "    print(f\"\\nNext word predictions for '{test_text}':\")\n",
    "    for word, prob in predictions:\n",
    "        print(f\"{word}: {prob:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d19c809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0bc5778",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstring\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Step 1: Preprocess the text data using WikiText\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mWikiTextPreprocessor\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Step 1: Preprocess the text data using WikiText\n",
    "class WikiTextPreprocessor:\n",
    "    def __init__(self, seq_length=10):\n",
    "        self.seq_length = seq_length\n",
    "        self.dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "        self.text = self._combine_dataset()\n",
    "        self.words = self._process_text()\n",
    "        self.word_counts = Counter(self.words)\n",
    "        self.vocab = sorted(self.word_counts, key=self.word_counts.get, reverse=True)\n",
    "        self.vocab_to_int = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.int_to_vocab = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.n_vocab = len(self.vocab)\n",
    "        \n",
    "    def _combine_dataset(self):\n",
    "        # Combine only the first 100 non-empty entries from training text\n",
    "        train_texts = [item for item in self.dataset['train']['text'] if item.strip()]\n",
    "        text = \"\\n\".join(train_texts[:100])\n",
    "        return text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    def _process_text(self):\n",
    "        return [word for word in self.text.split() if word]\n",
    "        \n",
    "    def create_sequences(self):\n",
    "        sequences = []\n",
    "        for i in range(len(self.words) - self.seq_length):\n",
    "            seq = self.words[i:i + self.seq_length]\n",
    "            next_word = self.words[i + self.seq_length]\n",
    "            sequences.append(([self.vocab_to_int[word] for word in seq], \n",
    "                            self.vocab_to_int[next_word]))\n",
    "        return sequences\n",
    "\n",
    "# Step 2: Create Dataset (same as before)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.sequences[idx]\n",
    "        return torch.tensor(sequence), torch.tensor([target])\n",
    "\n",
    "# Step 3: Define LSTM Model (same as before)\n",
    "class NextWordLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, n_layers=2):\n",
    "        super(NextWordLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])  # Only use the last output\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_().to(device),\n",
    "                  weight.new(self.lstm.num_layers, batch_size, self.lstm.hidden_size).zero_().to(device))\n",
    "        return hidden\n",
    "\n",
    "# Step 4: Training Function (same as before)\n",
    "def train_model(model, dataloader, epochs=10, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            batch_size = inputs.size(0)\n",
    "            hidden = model.init_hidden(batch_size, device)\n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            hidden = tuple([h.data for h in hidden])\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(outputs, targets.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Step 5: Prediction Function (same as before)\n",
    "def predict_next_word(model, preprocessor, initial_text, device, top_k=5):\n",
    "    model.eval()\n",
    "    initial_text = initial_text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    words = initial_text.split()[-preprocessor.seq_length:]\n",
    "    seq = [preprocessor.vocab_to_int.get(word, 0) for word in words]\n",
    "    seq = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "    \n",
    "    hidden = model.init_hidden(1, device)\n",
    "    with torch.no_grad():\n",
    "        outputs, _ = model(seq, hidden)\n",
    "        outputs = outputs[-1]\n",
    "        \n",
    "    probs = torch.softmax(outputs, dim=0)\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    top_words = [preprocessor.int_to_vocab[idx.item()] for idx in top_indices]\n",
    "    top_probs = top_probs.cpu().numpy()\n",
    "    \n",
    "    return list(zip(top_words, top_probs))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with WikiText\n",
    "    seq_length = 5\n",
    "    preprocessor = WikiTextPreprocessor(seq_length)\n",
    "    sequences = preprocessor.create_sequences()\n",
    "    \n",
    "    # Create dataloader\n",
    "    batch_size = 128  # Increased batch size for better training\n",
    "    dataset = TextDataset(sequences)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model with larger dimensions for WikiText\n",
    "    vocab_size = preprocessor.n_vocab\n",
    "    model = NextWordLSTM(vocab_size, embedding_dim=256, hidden_dim=512, n_layers=2)\n",
    "    \n",
    "    # Train model with more epochs\n",
    "    train_model(model, dataloader, epochs=30, lr=0.001)\n",
    "    \n",
    "    # Make predictions\n",
    "    device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "    num_predictions = 60  # Number of words to predict\n",
    "    current_text = \"the united states\"\n",
    "\n",
    "    for _ in range(num_predictions):\n",
    "        predictions = predict_next_word(model, preprocessor, current_text, device)\n",
    "        if predictions:\n",
    "            best_word, _ = max(predictions, key=lambda x: x[1])\n",
    "            current_text += \" \" + best_word\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nFinal sentence after {num_predictions} predictions:\")\n",
    "    print(current_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3eee61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf980ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final sentence after 60 predictions:\n",
      "the united states government and began a desperate but ultimately futile dispatch of letters and telegrams asking for reinforcements although rumors were widely spread that they were already coming the first telegraph wire to span between little rock and memphis had recently been completed local attorney john m harrel was asked to compose the first telegraph dispatched from arkansas s capital in his\n"
     ]
    }
   ],
   "source": [
    "num_predictions = 60  # Number of words to predict\n",
    "current_text = \"the united states\"\n",
    "\n",
    "for _ in range(num_predictions):\n",
    "    predictions = predict_next_word(model, preprocessor, current_text, device)\n",
    "    if predictions:\n",
    "        best_word, _ = max(predictions, key=lambda x: x[1])\n",
    "        current_text += \" \" + best_word\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"\\nFinal sentence after {num_predictions} predictions:\")\n",
    "print(current_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916d27b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\araut1\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at C:\\Users\\araut1\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\0.0.0\\b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Fri Apr  4 13:48:47 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 7.2463\n",
      "Epoch 2/40, Loss: 6.5955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 176\u001b[39m\n\u001b[32m    167\u001b[39m model = NextWordTransformer(\n\u001b[32m    168\u001b[39m     vocab_size, \n\u001b[32m    169\u001b[39m     embedding_dim=\u001b[32m256\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     dropout=\u001b[32m0.1\u001b[39m\n\u001b[32m    173\u001b[39m )\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m    179\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, epochs, lr)\u001b[39m\n\u001b[32m    127\u001b[39m     loss.backward()\n\u001b[32m    128\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m avg_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[32m    133\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "# Step 1: Preprocess the text data using WikiText (same as before)\n",
    "class WikiTextPreprocessor:\n",
    "    def __init__(self, seq_length=10):\n",
    "        self.seq_length = seq_length\n",
    "        self.dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "        self.text = self._combine_dataset()\n",
    "        self.words = self._process_text()\n",
    "        self.word_counts = Counter(self.words)\n",
    "        self.vocab = sorted(self.word_counts, key=self.word_counts.get, reverse=True)\n",
    "        self.vocab_to_int = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.int_to_vocab = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.n_vocab = len(self.vocab)\n",
    "        \n",
    "    def _combine_dataset(self):\n",
    "        # Combine only the first 100 non-empty entries from training text\n",
    "        train_texts = [item for item in self.dataset['train']['text'] if item.strip()]\n",
    "        text = \"\\n\".join(train_texts)\n",
    "        return text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    def _process_text(self):\n",
    "        return [word for word in self.text.split() if word]\n",
    "        \n",
    "    def create_sequences(self):\n",
    "        sequences = []\n",
    "        for i in range(len(self.words) - self.seq_length):\n",
    "            seq = self.words[i:i + self.seq_length]\n",
    "            next_word = self.words[i + self.seq_length]\n",
    "            sequences.append(([self.vocab_to_int[word] for word in seq], \n",
    "                            self.vocab_to_int[next_word]))\n",
    "        return sequences\n",
    "\n",
    "# Step 2: Create Dataset (same as before)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.sequences[idx]\n",
    "        return torch.tensor(sequence), torch.tensor([target])\n",
    "\n",
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Transformer Model\n",
    "class NextWordTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, nhead=8, num_layers=2, dropout=0.1):\n",
    "        super(NextWordTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, \n",
    "            nhead=nhead, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding + positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        # Generate a square causal mask for the sequence\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
    "        x = self.transformer_encoder(x, mask=mask)\n",
    "        \n",
    "        # Only use the last output for prediction\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "# Step 4: Training Function (modified for Transformer)\n",
    "def train_model(model, dataloader, epochs=10, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Step 5: Prediction Function (modified for Transformer)\n",
    "def predict_next_word(model, preprocessor, initial_text, device, top_k=5):\n",
    "    model.eval()\n",
    "    initial_text = initial_text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    words = initial_text.split()[-preprocessor.seq_length:]\n",
    "    seq = [preprocessor.vocab_to_int.get(word, 0) for word in words]\n",
    "    seq = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(seq)\n",
    "        \n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    top_words = [preprocessor.int_to_vocab[idx.item()] for idx in top_indices[0]]\n",
    "    top_probs = top_probs[0].cpu().numpy()\n",
    "    \n",
    "    return list(zip(top_words, top_probs))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with WikiText\n",
    "    seq_length = 40  # Longer sequence length works better with Transformers\n",
    "    preprocessor = WikiTextPreprocessor(seq_length)\n",
    "    sequences = preprocessor.create_sequences()\n",
    "    \n",
    "    # Create dataloader\n",
    "    batch_size = 128  # Adjusted batch size\n",
    "    dataset = TextDataset(sequences)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize Transformer model\n",
    "    vocab_size = preprocessor.n_vocab\n",
    "    model = NextWordTransformer(\n",
    "        vocab_size, \n",
    "        embedding_dim=256, \n",
    "        nhead=8, \n",
    "        num_layers=4, \n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    train_model(model, dataloader, epochs=40, lr=0.0001)\n",
    "    \n",
    "    # Make predictions\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    num_predictions = 60  # Number of words to predict\n",
    "    current_text = \"the united states\"\n",
    "\n",
    "    for _ in range(num_predictions):\n",
    "        predictions = predict_next_word(model, preprocessor, current_text, device)\n",
    "        if predictions:\n",
    "            best_word, _ = max(predictions, key=lambda x: x[1])\n",
    "            current_text += \" \" + best_word\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nFinal sentence after {num_predictions} predictions:\")\n",
    "    print(current_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52dc01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'preprocessor': preprocessor,\n",
    "}, 'transformer_word_predictor.pth')\n",
    "\n",
    "# Then later when you want to load and use it:\n",
    "def load_and_generate(initial_text, num_predictions=60, model_path='transformer_word_predictor.pth'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load the saved model and preprocessor\n",
    "    checkpoint = torch.load(model_path)\n",
    "    preprocessor = checkpoint['preprocessor']\n",
    "    \n",
    "    # Recreate the model architecture\n",
    "    model = NextWordTransformer(\n",
    "        preprocessor.n_vocab,\n",
    "        embedding_dim=256,\n",
    "        nhead=8,\n",
    "        num_layers=4,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate text\n",
    "    current_text = initial_text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    for _ in range(num_predictions):\n",
    "        predictions = predict_next_word(model, preprocessor, current_text, device)\n",
    "        if predictions:\n",
    "            best_word, _ = max(predictions, key=lambda x: x[1])\n",
    "            current_text += \" \" + best_word\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nGenerated text after {num_predictions} predictions:\")\n",
    "    print(current_text)\n",
    "\n",
    "# Example usage of loading and generating\n",
    "load_and_generate(\"the united states\", 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ac86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your-access-token' with your actual Hugging Face token\n",
    "login(\"your-access-token\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
