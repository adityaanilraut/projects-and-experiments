{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, mixed_precision\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable mixed precision to reduce memory usage\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Constants with reduced requirements\n",
    "IMG_SIZE = (256, 256)  # Reduced from 256x256\n",
    "BATCH_SIZE = 64        # Reduced from 32\n",
    "EPOCHS = 30\n",
    "DATASET_PATH = './'\n",
    "MODEL_PATH = 'saved_models'\n",
    "TEST_VIDEO = 'test_video.mp4'\n",
    "\n",
    "class MemoryEfficientDataset:\n",
    "    def __init__(self):\n",
    "        self.frame_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "    def build_frame_dataset(self, max_samples_per_class=5000):\n",
    "        \"\"\"Build dataset by saving frames to disk to avoid memory overload\"\"\"\n",
    "        os.makedirs('temp_frames/real', exist_ok=True)\n",
    "        os.makedirs('temp_frames/fake', exist_ok=True)\n",
    "        \n",
    "        # Process real videos\n",
    "        real_video_dir = os.path.join(DATASET_PATH, 'Celeb-real')\n",
    "        self._process_videos(real_video_dir, 'real', max_samples_per_class)\n",
    "        \n",
    "        # Process fake videos\n",
    "        fake_video_dir = os.path.join(DATASET_PATH, 'Celeb-synthesis')\n",
    "        self._process_videos(fake_video_dir, 'fake', max_samples_per_class)\n",
    "        \n",
    "        # Balance classes\n",
    "        min_samples = min(\n",
    "            len([x for x in self.labels if x == 0]),\n",
    "            len([x for x in self.labels if x == 1]))\n",
    "        self.frame_paths = self.frame_paths[:min_samples*2]\n",
    "        self.labels = self.labels[:min_samples*2]\n",
    "        \n",
    "    def _process_videos(self, video_dir, label_str, max_samples):\n",
    "        \"\"\"Process videos and save frames to disk\"\"\"\n",
    "        label = 0 if label_str == 'real' else 1\n",
    "        video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')][:max_samples//10]\n",
    "        \n",
    "        for video_file in tqdm(video_files, desc=f'Processing {label_str} videos'):\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            frame_count = 0\n",
    "            \n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            while cap.isOpened() and frame_count < 10:  # Max 10 frames per video\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                    \n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, IMG_SIZE)\n",
    "                \n",
    "                # Save frame to disk\n",
    "                frame_path = f'temp_frames/{label_str}/{video_file}_frame{frame_count}.jpg'\n",
    "                cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                \n",
    "                self.frame_paths.append(frame_path)\n",
    "                self.labels.append(label)\n",
    "                frame_count += 1\n",
    "                \n",
    "            cap.release()\n",
    "            \n",
    "    def get_generator(self, batch_size=16):\n",
    "        \"\"\"Create memory-efficient data generator\"\"\"\n",
    "        # Split paths and labels\n",
    "        train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "            self.frame_paths, self.labels, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Custom generator to load images on demand\n",
    "        def path_generator(paths, labels):\n",
    "            while True:\n",
    "                for i in range(0, len(paths), batch_size):\n",
    "                    batch_paths = paths[i:i+batch_size]\n",
    "                    batch_labels = labels[i:i+batch_size]\n",
    "                    \n",
    "                    batch_images = []\n",
    "                    for path in batch_paths:\n",
    "                        img = cv2.imread(path)\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        img = (img / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "                        batch_images.append(img)\n",
    "                    \n",
    "                    yield np.array(batch_images, dtype=np.float32), np.array(batch_labels, dtype=np.float32)\n",
    "        \n",
    "        train_steps = len(train_paths) // batch_size\n",
    "        test_steps = len(test_paths) // batch_size\n",
    "        \n",
    "        return path_generator(train_paths, train_labels), train_steps, \\\n",
    "               path_generator(test_paths, test_labels), test_steps\n",
    "\n",
    "class LiteDeepfakeDetector:\n",
    "    def __init__(self):\n",
    "        self.discriminator = self.build_lite_discriminator()\n",
    "        self.generator = self.build_lite_generator()\n",
    "        self.gan = self.build_lite_gan()\n",
    "        \n",
    "    def build_lite_generator(self):\n",
    "        \"\"\"Memory-efficient generator with depthwise convolutions\"\"\"\n",
    "        inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
    "        \n",
    "        # Downsample\n",
    "        x = layers.Conv2D(32, (4,4), strides=2, padding='same')(inputs)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.SeparableConv2D(64, (4,4), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.SeparableConv2D(128, (4,4), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = layers.SeparableConv2D(256, (4,4), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        # Upsample\n",
    "        x = layers.Conv2DTranspose(128, (4,4), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        \n",
    "        x = layers.Conv2DTranspose(64, (4,4), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        \n",
    "        x = layers.Conv2DTranspose(32, (4,4), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        \n",
    "        outputs = layers.Conv2DTranspose(3, (4,4), strides=2, padding='same', \n",
    "                                       activation='tanh')(x)\n",
    "        \n",
    "        return models.Model(inputs, outputs)\n",
    "    \n",
    "    def build_lite_discriminator(self):\n",
    "        \"\"\"Memory-efficient discriminator with separable convolutions\"\"\"\n",
    "        inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
    "        \n",
    "        x = layers.SeparableConv2D(32, (4,4), strides=2, padding='same')(inputs)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.SeparableConv2D(64, (4,4), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.SeparableConv2D(128, (4,4), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.SeparableConv2D(256, (4,4), strides=1, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        # PatchGAN output\n",
    "        patch_output = layers.Conv2D(1, (4,4), padding='same')(x)\n",
    "        \n",
    "        # Global pooling for classification\n",
    "        y = layers.GlobalAveragePooling2D()(x)\n",
    "        y = layers.Dense(1, activation='sigmoid')(y)\n",
    "        \n",
    "        return models.Model(inputs, [patch_output, y])\n",
    "    \n",
    "    def build_lite_gan(self):\n",
    "        \"\"\"Lite GAN model\"\"\"\n",
    "        self.discriminator.trainable = False\n",
    "        img_input = layers.Input(shape=(*IMG_SIZE, 3))\n",
    "        generated_img = self.generator(img_input)\n",
    "        gan_output, class_output = self.discriminator(generated_img)\n",
    "        return models.Model(img_input, [gan_output, class_output])\n",
    "    \n",
    "    def compile_models(self, lr=0.0002):\n",
    "        \"\"\"Compile models with memory optimizations\"\"\"\n",
    "        opt = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
    "        \n",
    "        self.discriminator.compile(\n",
    "            optimizer=opt,\n",
    "            loss=['mse', 'binary_crossentropy'],\n",
    "            loss_weights=[0.5, 0.5]\n",
    "        )\n",
    "        \n",
    "        self.generator.compile(\n",
    "            optimizer=opt,\n",
    "            loss='mae'\n",
    "        )\n",
    "        \n",
    "        self.gan.compile(\n",
    "            optimizer=opt,\n",
    "            loss=['mse', 'binary_crossentropy']\n",
    "        )\n",
    "    \n",
    "    def train(self, train_gen, train_steps, test_gen, test_steps, epochs=EPOCHS):\n",
    "        \"\"\"Memory-efficient training\"\"\"\n",
    "        real_labels = np.ones((BATCH_SIZE, *self.discriminator.output_shape[0][1:]))\n",
    "        fake_labels = np.zeros((BATCH_SIZE, *self.discriminator.output_shape[0][1:]))\n",
    "        \n",
    "        detection_real = np.zeros((BATCH_SIZE, 1))\n",
    "        detection_fake = np.ones((BATCH_SIZE, 1))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            # Train discriminator\n",
    "            d_losses = []\n",
    "            for _ in tqdm(range(train_steps), desc=\"Training discriminator\"):\n",
    "                real_imgs, _ = next(train_gen)\n",
    "                \n",
    "                # Generate fake images\n",
    "                noise = tf.random.normal(real_imgs.shape)\n",
    "                fake_imgs = self.generator.predict(noise, verbose=0)\n",
    "                \n",
    "                # Train on real images\n",
    "                d_loss_real = self.discriminator.train_on_batch(\n",
    "                    real_imgs, [real_labels[:len(real_imgs)], detection_real[:len(real_imgs)]])\n",
    "                \n",
    "                # Train on fake images\n",
    "                d_loss_fake = self.discriminator.train_on_batch(\n",
    "                    fake_imgs, [fake_labels[:len(fake_imgs)], detection_fake[:len(fake_imgs)]])\n",
    "                \n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                d_losses.append(d_loss)\n",
    "            \n",
    "            # Train generator\n",
    "            g_losses = []\n",
    "            for _ in tqdm(range(train_steps), desc=\"Training generator\"):\n",
    "                real_imgs, _ = next(train_gen)\n",
    "                noise = tf.random.normal(real_imgs.shape)\n",
    "                g_loss = self.gan.train_on_batch(\n",
    "                    noise, [real_labels[:len(real_imgs)], detection_fake[:len(real_imgs)]])\n",
    "                g_losses.append(g_loss)\n",
    "            \n",
    "            # Evaluate\n",
    "            avg_d_loss = np.mean(d_losses, axis=0)\n",
    "            avg_g_loss = np.mean(g_losses, axis=0)\n",
    "            val_acc = self.evaluate(test_gen, test_steps)\n",
    "            \n",
    "            print(f\"Discriminator Loss: {avg_d_loss[0]:.4f} | Generator Loss: {avg_g_loss[0]:.4f}\")\n",
    "            print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    def evaluate(self, test_gen, test_steps):\n",
    "        \"\"\"Memory-efficient evaluation\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for _ in range(test_steps):\n",
    "            imgs, labels = next(test_gen)\n",
    "            _, preds = self.discriminator.predict(imgs, verbose=0)\n",
    "            preds = (preds > 0.5).astype(int)\n",
    "            correct += np.sum(preds.flatten() == labels)\n",
    "            total += len(labels)\n",
    "            \n",
    "        return correct / total\n",
    "    \n",
    "    def save_models(self):\n",
    "        \"\"\"Save models with reduced precision\"\"\"\n",
    "        os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "        \n",
    "        # Save weights only to reduce file size\n",
    "        self.generator.save_weights(os.path.join(MODEL_PATH, 'generator_weights.h5'))\n",
    "        self.discriminator.save_weights(os.path.join(MODEL_PATH, 'discriminator_weights.h5'))\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load models with architecture recreation\"\"\"\n",
    "        self.generator = self.build_lite_generator()\n",
    "        self.discriminator = self.build_lite_discriminator()\n",
    "        self.gan = self.build_lite_gan()\n",
    "        \n",
    "        self.generator.load_weights(os.path.join(MODEL_PATH, 'generator_weights.h5'))\n",
    "        self.discriminator.load_weights(os.path.join(MODEL_PATH, 'discriminator_weights.h5'))\n",
    "\n",
    "class EfficientVideoTester:\n",
    "    def __init__(self, detector):\n",
    "        self.detector = detector\n",
    "        self.face_cascade = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process single frame with memory efficiency\"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            x, y, w, h = faces[0]\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            face = cv2.resize(face, IMG_SIZE)\n",
    "            face = (face / 127.5) - 1.0  # Normalize in-place\n",
    "            return face\n",
    "        return None\n",
    "    \n",
    "    def test_video(self, video_path, threshold=0.7, frame_interval=5):\n",
    "        \"\"\"Test video with memory-efficient frame processing\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_results = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if frame_count % frame_interval == 0:  # Process every nth frame\n",
    "                face = self.process_frame(frame)\n",
    "                if face is not None:\n",
    "                    face = np.expand_dims(face, axis=0)\n",
    "                    _, detection_prob = self.detector.discriminator.predict(face, verbose=0)\n",
    "                    frame_results.append(detection_prob[0][0])\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "        cap.release()\n",
    "        \n",
    "        if not frame_results:\n",
    "            print(\"No faces detected in video\")\n",
    "            return False, 0.0\n",
    "        \n",
    "        avg_prob = np.mean(frame_results)\n",
    "        is_fake = avg_prob > threshold\n",
    "        confidence = avg_prob if is_fake else 1 - avg_prob\n",
    "        \n",
    "        print(\"\\nMemory-Efficient Test Results:\")\n",
    "        print(f\"Frames processed: {len(frame_results)}/{frame_count}\")\n",
    "        print(f\"Average score: {avg_prob:.4f}\")\n",
    "        print(f\"Conclusion: {'FAKE' if is_fake else 'REAL'} (confidence: {confidence*100:.2f}%)\")\n",
    "        \n",
    "        return is_fake, confidence\n",
    "\n",
    "def main():\n",
    "    # Initialize dataset\n",
    "    print(\"Building memory-efficient dataset...\")\n",
    "    dataset = MemoryEfficientDataset()\n",
    "    dataset.build_frame_dataset(max_samples_per_class=3000)  # Reduced sample count\n",
    "    \n",
    "    # Get generators\n",
    "    train_gen, train_steps, test_gen, test_steps = dataset.get_generator(BATCH_SIZE)\n",
    "    \n",
    "    # Initialize and train model\n",
    "    detector = LiteDeepfakeDetector()\n",
    "    detector.compile_models()\n",
    "    \n",
    "    print(\"\\nTraining memory-efficient model...\")\n",
    "    detector.train(train_gen, train_steps, test_gen, test_steps)\n",
    "    detector.save_models()\n",
    "    \n",
    "    # Test video\n",
    "    print(\"\\nTesting video with memory optimizations...\")\n",
    "    tester = EfficientVideoTester(detector)\n",
    "    is_fake, confidence = tester.test_video(TEST_VIDEO)\n",
    "    \n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(f\"Video '{TEST_VIDEO}' is classified as: {'FAKE' if is_fake else 'REAL'}\")\n",
    "    print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clear session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a18c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building enhanced dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real videos: 100%|██████████| 500/500 [00:20<00:00, 24.86it/s]\n",
      "Processing real videos: 100%|██████████| 500/500 [00:20<00:00, 24.86it/s]\n",
      "Processing fake videos: 100%|██████████| 500/500 [00:20<00:00, 23.82it/s]\n",
      "C:\\Users\\araut1\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "\n",
      "C:\\Users\\araut1\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">942</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">167</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">314</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">167</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">314</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>,   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │ leaky_re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">819,456</span> │ leaky_re_lu_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> │ leaky_re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ leaky_re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ leaky_re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,816</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ leaky_re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classification      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m942\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m167\u001b[0m, \u001b[38;5;34m314\u001b[0m,  │      \u001b[38;5;34m4,864\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m167\u001b[0m, \u001b[38;5;34m314\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m105\u001b[0m,   │    \u001b[38;5;34m204,928\u001b[0m │ leaky_re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m105\u001b[0m,   │        \u001b[38;5;34m512\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m105\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m53\u001b[0m,    │    \u001b[38;5;34m819,456\u001b[0m │ leaky_re_lu_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m53\u001b[0m,    │      \u001b[38;5;34m1,024\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m53\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m27\u001b[0m,    │  \u001b[38;5;34m2,097,664\u001b[0m │ leaky_re_lu_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m27\u001b[0m,    │      \u001b[38;5;34m2,048\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m27\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m27\u001b[0m,    │    \u001b[38;5;34m262,656\u001b[0m │ leaky_re_lu_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply (\u001b[38;5;33mMultiply\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m27\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ leaky_re_lu_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m)              │            │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m27\u001b[0m,    │  \u001b[38;5;34m4,194,816\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m27\u001b[0m,    │      \u001b[38;5;34m2,048\u001b[0m │ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ leaky_re_lu_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m27\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ leaky_re_lu_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m65,664\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classification      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,655,809</span> (29.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,655,809\u001b[0m (29.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,652,993</span> (29.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,652,993\u001b[0m (29.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> (11.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,816\u001b[0m (11.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training enhanced model...\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5690 - loss: 0.7232\n",
      "Epoch 1: val_accuracy improved from -inf to 0.66179, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.66179, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m934s\u001b[0m 2s/step - accuracy: 0.5691 - loss: 0.7232 - val_accuracy: 0.6618 - val_loss: 0.6147 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "Epoch 2/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6658 - loss: 0.6066\n",
      "Epoch 2: val_accuracy improved from 0.66179 to 0.75554, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.66179 to 0.75554, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2124s\u001b[0m 4s/step - accuracy: 0.6659 - loss: 0.6066 - val_accuracy: 0.7555 - val_loss: 0.5176 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "Epoch 3/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.7565 - loss: 0.5174\n",
      "Epoch 3: val_accuracy improved from 0.75554 to 0.78226, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.75554 to 0.78226, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5430s\u001b[0m 11s/step - accuracy: 0.7565 - loss: 0.5174 - val_accuracy: 0.7823 - val_loss: 0.4932 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "Epoch 4/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.7965 - loss: 0.4630 \n",
      "Epoch 4: val_accuracy did not improve from 0.78226\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5648s\u001b[0m 11s/step - accuracy: 0.7966 - loss: 0.4630 - val_accuracy: 0.7087 - val_loss: 0.5546 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.78226\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5648s\u001b[0m 11s/step - accuracy: 0.7966 - loss: 0.4630 - val_accuracy: 0.7087 - val_loss: 0.5546 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.8215 - loss: 0.4234\n",
      "Epoch 5: val_accuracy did not improve from 0.78226\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4606s\u001b[0m 9s/step - accuracy: 0.8215 - loss: 0.4233 - val_accuracy: 0.6825 - val_loss: 0.7307 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.78226\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4606s\u001b[0m 9s/step - accuracy: 0.8215 - loss: 0.4233 - val_accuracy: 0.6825 - val_loss: 0.7307 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8515 - loss: 0.3659\n",
      "Epoch 6: val_accuracy improved from 0.78226 to 0.80515, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.78226 to 0.80515, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3024s\u001b[0m 6s/step - accuracy: 0.8515 - loss: 0.3659 - val_accuracy: 0.8051 - val_loss: 0.4571 - learning_rate: 9.0000e-05\n",
      "Epoch 7/50\n",
      "Epoch 7/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8540 - loss: 0.3722\n",
      "Epoch 7: val_accuracy improved from 0.80515 to 0.88036, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.80515 to 0.88036, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2956s\u001b[0m 6s/step - accuracy: 0.8540 - loss: 0.3722 - val_accuracy: 0.8804 - val_loss: 0.3236 - learning_rate: 9.0000e-05\n",
      "Epoch 8/50\n",
      "Epoch 8/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8612 - loss: 0.3445\n",
      "Epoch 8: val_accuracy did not improve from 0.88036\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2931s\u001b[0m 6s/step - accuracy: 0.8612 - loss: 0.3445 - val_accuracy: 0.8657 - val_loss: 0.3389 - learning_rate: 9.0000e-05\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.88036\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2931s\u001b[0m 6s/step - accuracy: 0.8612 - loss: 0.3445 - val_accuracy: 0.8657 - val_loss: 0.3389 - learning_rate: 9.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8687 - loss: 0.3417\n",
      "Epoch 9: val_accuracy improved from 0.88036 to 0.89197, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.88036 to 0.89197, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2914s\u001b[0m 6s/step - accuracy: 0.8688 - loss: 0.3417 - val_accuracy: 0.8920 - val_loss: 0.3122 - learning_rate: 9.0000e-05\n",
      "Epoch 10/50\n",
      "Epoch 10/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.8790 - loss: 0.3233\n",
      "Epoch 10: val_accuracy did not improve from 0.89197\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3067s\u001b[0m 6s/step - accuracy: 0.8790 - loss: 0.3233 - val_accuracy: 0.8738 - val_loss: 0.3633 - learning_rate: 9.0000e-05\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.89197\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3067s\u001b[0m 6s/step - accuracy: 0.8790 - loss: 0.3233 - val_accuracy: 0.8738 - val_loss: 0.3633 - learning_rate: 9.0000e-05\n",
      "Epoch 11/50\n",
      "Epoch 11/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8871 - loss: 0.3021\n",
      "Epoch 11: val_accuracy improved from 0.89197 to 0.89652, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.89197 to 0.89652, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2933s\u001b[0m 6s/step - accuracy: 0.8871 - loss: 0.3021 - val_accuracy: 0.8965 - val_loss: 0.2928 - learning_rate: 8.1000e-05\n",
      "Epoch 12/50\n",
      "Epoch 12/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8869 - loss: 0.2927\n",
      "Epoch 12: val_accuracy improved from 0.89652 to 0.90358, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.89652 to 0.90358, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2925s\u001b[0m 6s/step - accuracy: 0.8869 - loss: 0.2927 - val_accuracy: 0.9036 - val_loss: 0.2782 - learning_rate: 8.1000e-05\n",
      "Epoch 13/50\n",
      "Epoch 13/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8988 - loss: 0.2719\n",
      "Epoch 13: val_accuracy did not improve from 0.90358\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.90358\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2936s\u001b[0m 6s/step - accuracy: 0.8988 - loss: 0.2719 - val_accuracy: 0.8975 - val_loss: 0.2808 - learning_rate: 8.1000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2936s\u001b[0m 6s/step - accuracy: 0.8988 - loss: 0.2719 - val_accuracy: 0.8975 - val_loss: 0.2808 - learning_rate: 8.1000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8879 - loss: 0.2872\n",
      "Epoch 14: val_accuracy improved from 0.90358 to 0.91065, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.90358 to 0.91065, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2925s\u001b[0m 6s/step - accuracy: 0.8879 - loss: 0.2871 - val_accuracy: 0.9107 - val_loss: 0.2698 - learning_rate: 8.1000e-05\n",
      "Epoch 15/50\n",
      "Epoch 15/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9013 - loss: 0.2745\n",
      "Epoch 15: val_accuracy did not improve from 0.91065\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2954s\u001b[0m 6s/step - accuracy: 0.9013 - loss: 0.2745 - val_accuracy: 0.8713 - val_loss: 0.3160 - learning_rate: 8.1000e-05\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.91065\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2954s\u001b[0m 6s/step - accuracy: 0.9013 - loss: 0.2745 - val_accuracy: 0.8713 - val_loss: 0.3160 - learning_rate: 8.1000e-05\n",
      "Epoch 16/50\n",
      "Epoch 16/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9063 - loss: 0.2665\n",
      "Epoch 16: val_accuracy did not improve from 0.91065\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2955s\u001b[0m 6s/step - accuracy: 0.9063 - loss: 0.2665 - val_accuracy: 0.8718 - val_loss: 0.3246 - learning_rate: 7.2900e-05\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.91065\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2955s\u001b[0m 6s/step - accuracy: 0.9063 - loss: 0.2665 - val_accuracy: 0.8718 - val_loss: 0.3246 - learning_rate: 7.2900e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9002 - loss: 0.2657\n",
      "Epoch 17: val_accuracy did not improve from 0.91065\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3035s\u001b[0m 6s/step - accuracy: 0.9002 - loss: 0.2657 - val_accuracy: 0.8995 - val_loss: 0.2891 - learning_rate: 7.2900e-05\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.91065\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3035s\u001b[0m 6s/step - accuracy: 0.9002 - loss: 0.2657 - val_accuracy: 0.8995 - val_loss: 0.2891 - learning_rate: 7.2900e-05\n",
      "Epoch 18/50\n",
      "Epoch 18/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9105 - loss: 0.2558\n",
      "Epoch 18: val_accuracy did not improve from 0.91065\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2975s\u001b[0m 6s/step - accuracy: 0.9105 - loss: 0.2558 - val_accuracy: 0.8910 - val_loss: 0.2989 - learning_rate: 7.2900e-05\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.91065\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2975s\u001b[0m 6s/step - accuracy: 0.9105 - loss: 0.2558 - val_accuracy: 0.8910 - val_loss: 0.2989 - learning_rate: 7.2900e-05\n",
      "Epoch 19/50\n",
      "Epoch 19/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9017 - loss: 0.2630\n",
      "Epoch 19: val_accuracy improved from 0.91065 to 0.91217, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.91065 to 0.91217, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2974s\u001b[0m 6s/step - accuracy: 0.9017 - loss: 0.2630 - val_accuracy: 0.9122 - val_loss: 0.2721 - learning_rate: 7.2900e-05\n",
      "Epoch 20/50\n",
      "Epoch 20/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9114 - loss: 0.2447\n",
      "Epoch 20: val_accuracy did not improve from 0.91217\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2968s\u001b[0m 6s/step - accuracy: 0.9114 - loss: 0.2447 - val_accuracy: 0.8809 - val_loss: 0.3055 - learning_rate: 7.2900e-05\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.91217\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2968s\u001b[0m 6s/step - accuracy: 0.9114 - loss: 0.2447 - val_accuracy: 0.8809 - val_loss: 0.3055 - learning_rate: 7.2900e-05\n",
      "Epoch 21/50\n",
      "Epoch 21/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9137 - loss: 0.2403\n",
      "Epoch 21: val_accuracy did not improve from 0.91217\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2987s\u001b[0m 6s/step - accuracy: 0.9136 - loss: 0.2403 - val_accuracy: 0.9076 - val_loss: 0.2604 - learning_rate: 6.5610e-05\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.91217\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2987s\u001b[0m 6s/step - accuracy: 0.9136 - loss: 0.2403 - val_accuracy: 0.9076 - val_loss: 0.2604 - learning_rate: 6.5610e-05\n",
      "Epoch 22/50\n",
      "Epoch 22/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9087 - loss: 0.2509\n",
      "Epoch 22: val_accuracy did not improve from 0.91217\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1574s\u001b[0m 3s/step - accuracy: 0.9087 - loss: 0.2509 - val_accuracy: 0.9046 - val_loss: 0.2403 - learning_rate: 6.5610e-05\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.91217\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1574s\u001b[0m 3s/step - accuracy: 0.9087 - loss: 0.2509 - val_accuracy: 0.9046 - val_loss: 0.2403 - learning_rate: 6.5610e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9135 - loss: 0.2485\n",
      "Epoch 23: val_accuracy improved from 0.91217 to 0.91822, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.91217 to 0.91822, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 1s/step - accuracy: 0.9135 - loss: 0.2485 - val_accuracy: 0.9182 - val_loss: 0.2346 - learning_rate: 6.5610e-05\n",
      "Epoch 24/50\n",
      "Epoch 24/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9148 - loss: 0.2434\n",
      "Epoch 24: val_accuracy did not improve from 0.91822\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m710s\u001b[0m 1s/step - accuracy: 0.9148 - loss: 0.2434 - val_accuracy: 0.8995 - val_loss: 0.2728 - learning_rate: 6.5610e-05\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.91822\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m710s\u001b[0m 1s/step - accuracy: 0.9148 - loss: 0.2434 - val_accuracy: 0.8995 - val_loss: 0.2728 - learning_rate: 6.5610e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9169 - loss: 0.2344\n",
      "Epoch 25: val_accuracy did not improve from 0.91822\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 1s/step - accuracy: 0.9169 - loss: 0.2344 - val_accuracy: 0.9096 - val_loss: 0.2725 - learning_rate: 6.5610e-05\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.91822\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 1s/step - accuracy: 0.9169 - loss: 0.2344 - val_accuracy: 0.9096 - val_loss: 0.2725 - learning_rate: 6.5610e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9230 - loss: 0.2173\n",
      "Epoch 26: val_accuracy did not improve from 0.91822\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 1s/step - accuracy: 0.9230 - loss: 0.2173 - val_accuracy: 0.8809 - val_loss: 0.3157 - learning_rate: 5.9049e-05\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.91822\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 1s/step - accuracy: 0.9230 - loss: 0.2173 - val_accuracy: 0.8809 - val_loss: 0.3157 - learning_rate: 5.9049e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9148 - loss: 0.2435\n",
      "Epoch 27: val_accuracy did not improve from 0.91822\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 1s/step - accuracy: 0.9148 - loss: 0.2434 - val_accuracy: 0.9056 - val_loss: 0.2759 - learning_rate: 5.9049e-05\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.91822\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 1s/step - accuracy: 0.9148 - loss: 0.2434 - val_accuracy: 0.9056 - val_loss: 0.2759 - learning_rate: 5.9049e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9220 - loss: 0.2217\n",
      "Epoch 28: val_accuracy improved from 0.91822 to 0.91974, saving model to saved_models\\discriminator.h5\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.91822 to 0.91974, saving model to saved_models\\discriminator.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 1s/step - accuracy: 0.9220 - loss: 0.2217 - val_accuracy: 0.9197 - val_loss: 0.2537 - learning_rate: 5.9049e-05\n",
      "Epoch 29/50\n",
      "Epoch 29/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9206 - loss: 0.2261\n",
      "Epoch 29: val_accuracy did not improve from 0.91974\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 1s/step - accuracy: 0.9206 - loss: 0.2261 - val_accuracy: 0.8945 - val_loss: 0.2691 - learning_rate: 5.9049e-05\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.91974\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 1s/step - accuracy: 0.9206 - loss: 0.2261 - val_accuracy: 0.8945 - val_loss: 0.2691 - learning_rate: 5.9049e-05\n",
      "Epoch 30/50\n",
      "Epoch 30/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9134 - loss: 0.2453\n",
      "Epoch 30: val_accuracy did not improve from 0.91974\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 1s/step - accuracy: 0.9134 - loss: 0.2452 - val_accuracy: 0.9107 - val_loss: 0.2503 - learning_rate: 5.9049e-05\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.91974\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 1s/step - accuracy: 0.9134 - loss: 0.2452 - val_accuracy: 0.9107 - val_loss: 0.2503 - learning_rate: 5.9049e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9215 - loss: 0.2207\n",
      "Epoch 31: val_accuracy did not improve from 0.91974\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 1s/step - accuracy: 0.9215 - loss: 0.2207 - val_accuracy: 0.9147 - val_loss: 0.2389 - learning_rate: 5.3144e-05\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.91974\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 1s/step - accuracy: 0.9215 - loss: 0.2207 - val_accuracy: 0.9147 - val_loss: 0.2389 - learning_rate: 5.3144e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9169 - loss: 0.2277\n",
      "Epoch 32: val_accuracy did not improve from 0.91974\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 1s/step - accuracy: 0.9169 - loss: 0.2277 - val_accuracy: 0.9147 - val_loss: 0.2261 - learning_rate: 5.3144e-05\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.91974\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 1s/step - accuracy: 0.9169 - loss: 0.2277 - val_accuracy: 0.9147 - val_loss: 0.2261 - learning_rate: 5.3144e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9280 - loss: 0.2127\n",
      "Epoch 33: val_accuracy did not improve from 0.91974\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 1s/step - accuracy: 0.9279 - loss: 0.2127 - val_accuracy: 0.9132 - val_loss: 0.2372 - learning_rate: 5.3144e-05\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.91974\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 1s/step - accuracy: 0.9279 - loss: 0.2127 - val_accuracy: 0.9132 - val_loss: 0.2372 - learning_rate: 5.3144e-05\n",
      "\n",
      "Testing video with enhanced model...\n",
      "\n",
      "Testing video with enhanced model...\n",
      "\n",
      "Memory-Efficient Test Results:\n",
      "Frames processed: 61/303\n",
      "Average score: 0.9589\n",
      "Conclusion: FAKE (confidence: 95.89%)\n",
      "\n",
      "Final Result:\n",
      "Video 'test_video.mp4' is classified as: FAKE\n",
      "Confidence: 95.89%\n",
      "\n",
      "Memory-Efficient Test Results:\n",
      "Frames processed: 61/303\n",
      "Average score: 0.9589\n",
      "Conclusion: FAKE (confidence: 95.89%)\n",
      "\n",
      "Final Result:\n",
      "Video 'test_video.mp4' is classified as: FAKE\n",
      "Confidence: 95.89%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, mixed_precision\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable mixed precision to reduce memory usage while maintaining model size\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Constants - updated to new frame size\n",
    "IMG_SIZE = (500, 942)  # Updated to 942×500 (height, width)\n",
    "BATCH_SIZE = 16        # Reduced batch size further to accommodate larger images\n",
    "EPOCHS = 50            # Increased epochs for better training\n",
    "DATASET_PATH = './'\n",
    "MODEL_PATH = 'saved_models'\n",
    "TEST_VIDEO = 'test_video.mp4'\n",
    "\n",
    "class MemoryEfficientDataset:\n",
    "    def __init__(self):\n",
    "        self.frame_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "    def build_frame_dataset(self, max_samples_per_class=5000):\n",
    "        \"\"\"Build dataset by saving frames to disk to avoid memory overload\"\"\"\n",
    "        os.makedirs('temp_frames/real', exist_ok=True)\n",
    "        os.makedirs('temp_frames/fake', exist_ok=True)\n",
    "        \n",
    "        # Process real videos\n",
    "        real_video_dir = os.path.join(DATASET_PATH, 'Celeb-real')\n",
    "        self._process_videos(real_video_dir, 'real', max_samples_per_class)\n",
    "        \n",
    "        # Process fake videos\n",
    "        fake_video_dir = os.path.join(DATASET_PATH, 'Celeb-synthesis')\n",
    "        self._process_videos(fake_video_dir, 'fake', max_samples_per_class)\n",
    "        \n",
    "        # Balance classes\n",
    "        min_samples = min(\n",
    "            len([x for x in self.labels if x == 0]),\n",
    "            len([x for x in self.labels if x == 1]))\n",
    "        self.frame_paths = self.frame_paths[:min_samples*2]\n",
    "        self.labels = self.labels[:min_samples*2]\n",
    "        \n",
    "    def _process_videos(self, video_dir, label_str, max_samples):\n",
    "        \"\"\"Process videos and save frames to disk\"\"\"\n",
    "        label = 0 if label_str == 'real' else 1\n",
    "        video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')][:max_samples//10]\n",
    "        \n",
    "        for video_file in tqdm(video_files, desc=f'Processing {label_str} videos'):\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            frame_count = 0\n",
    "            \n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            while cap.isOpened() and frame_count < 10:  # Max 10 frames per video\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                    \n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (IMG_SIZE[1], IMG_SIZE[0]))  # Updated to new size (width, height)\n",
    "                \n",
    "                # Save frame to disk\n",
    "                frame_path = f'temp_frames/{label_str}/{video_file}_frame{frame_count}.jpg'\n",
    "                cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                \n",
    "                self.frame_paths.append(frame_path)\n",
    "                self.labels.append(label)\n",
    "                frame_count += 1\n",
    "                \n",
    "            cap.release()\n",
    "            \n",
    "    def get_generator(self, batch_size=16):\n",
    "        \"\"\"Create memory-efficient data generator\"\"\"\n",
    "        # Split paths and labels\n",
    "        train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "            self.frame_paths, self.labels, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Add data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=15,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.1,\n",
    "            preprocessing_function=lambda x: (x / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "        )\n",
    "        \n",
    "        # Custom generator to load images on demand with augmentation\n",
    "        def path_generator(paths, labels):\n",
    "            while True:\n",
    "                # Shuffle indices for each epoch\n",
    "                indices = np.arange(len(paths))\n",
    "                np.random.shuffle(indices)\n",
    "                shuffled_paths = [paths[i] for i in indices]\n",
    "                shuffled_labels = [labels[i] for i in indices]\n",
    "                \n",
    "                for i in range(0, len(shuffled_paths), batch_size):\n",
    "                    batch_paths = shuffled_paths[i:i+batch_size]\n",
    "                    batch_labels = shuffled_labels[i:i+batch_size]\n",
    "                    \n",
    "                    batch_images = []\n",
    "                    for path in batch_paths:\n",
    "                        img = cv2.imread(path)\n",
    "                        if img is None:\n",
    "                            print(f\"Warning: Could not read image {path}\")\n",
    "                            continue\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        batch_images.append(img)\n",
    "                    \n",
    "                    if not batch_images:\n",
    "                        continue\n",
    "                    \n",
    "                    # Apply augmentation\n",
    "                    batch_images = next(datagen.flow(\n",
    "                        np.array(batch_images, dtype=np.float32),\n",
    "                        shuffle=False,\n",
    "                        batch_size=len(batch_images)))\n",
    "                    \n",
    "                    yield batch_images, np.array(batch_labels, dtype=np.float32)\n",
    "        \n",
    "        train_steps = max(1, len(train_paths) // batch_size)\n",
    "        test_steps = max(1, len(test_paths) // batch_size)\n",
    "        \n",
    "        return path_generator(train_paths, train_labels), train_steps, \\\n",
    "               path_generator(test_paths, test_labels), test_steps\n",
    "\n",
    "class EnhancedDeepfakeDetector:\n",
    "    def __init__(self):\n",
    "        # First create the discriminator standalone model - this is what we'll train\n",
    "        self.discriminator = self.build_enhanced_discriminator()\n",
    "        \n",
    "        # The GAN model will only be used for inference so we build it last\n",
    "        self.discriminator.summary()\n",
    "    \n",
    "    def build_enhanced_discriminator(self):\n",
    "        \"\"\"Enhanced discriminator with more capacity and attention\"\"\"\n",
    "        inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
    "        \n",
    "        # Enhanced feature extraction with adjusted strides for larger images\n",
    "        x = layers.Conv2D(64, (5,5), strides=3, padding='same')(inputs)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Conv2D(128, (5,5), strides=3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Conv2D(256, (5,5), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Conv2D(512, (4,4), strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention = layers.Conv2D(512, (1,1), activation='sigmoid')(x)\n",
    "        x = layers.multiply([x, attention])\n",
    "        \n",
    "        # Additional convolutional layers for better feature extraction\n",
    "        x = layers.Conv2D(512, (4,4), strides=1, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        # Enhanced classification head\n",
    "        y = layers.GlobalAveragePooling2D()(x)\n",
    "        y = layers.Dense(128, activation='relu')(y)\n",
    "        y = layers.Dropout(0.3)(y)\n",
    "        class_output = layers.Dense(1, activation='sigmoid', name='classification')(y)\n",
    "        \n",
    "        return models.Model(inputs, class_output)\n",
    "    \n",
    "    def compile_model(self, lr=0.0001):\n",
    "        \"\"\"Compile the discriminator model with appropriate optimizer\"\"\"\n",
    "        opt = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
    "        \n",
    "        self.discriminator.compile(\n",
    "            optimizer=opt,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def train(self, train_gen, train_steps, test_gen, test_steps, epochs=EPOCHS):\n",
    "        \"\"\"Enhanced training with callbacks and learning rate scheduling\"\"\"\n",
    "        # Learning rate scheduler\n",
    "        lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(\n",
    "            lambda epoch, lr: lr * 0.9 if epoch > 0 and epoch % 5 == 0 else lr\n",
    "        )\n",
    "        \n",
    "        # Early stopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # ModelCheckpoint to save best model\n",
    "        os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(MODEL_PATH, 'discriminator.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='logs',\n",
    "            write_graph=True,\n",
    "            update_freq='epoch'\n",
    "        )\n",
    "        \n",
    "        history = self.discriminator.fit(\n",
    "            train_gen,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=epochs,\n",
    "            validation_data=test_gen,\n",
    "            validation_steps=test_steps,\n",
    "            callbacks=[lr_scheduler_callback, early_stopping, checkpoint, tensorboard]\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        self.plot_training_history(history)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, test_gen, test_steps):\n",
    "        \"\"\"Evaluate the model\"\"\"\n",
    "        metrics = self.discriminator.evaluate(test_gen, steps=test_steps)\n",
    "        print(f\"Test Loss: {metrics[0]:.4f}, Test Accuracy: {metrics[1]:.4f}\")\n",
    "        return metrics[1]  # Return accuracy\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\"Save the model\"\"\"\n",
    "        os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "        self.discriminator.save(os.path.join(MODEL_PATH, 'discriminator.h5'))\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load saved model\"\"\"\n",
    "        self.discriminator = tf.keras.models.load_model(os.path.join(MODEL_PATH, 'discriminator.h5'))\n",
    "\n",
    "class EfficientVideoTester:\n",
    "    def __init__(self, detector):\n",
    "        self.detector = detector\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process single frame with memory efficiency\"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (IMG_SIZE[1], IMG_SIZE[0]))  # Updated to new size\n",
    "        frame = (frame / 127.5) - 1.0  # Normalize in-place\n",
    "        return frame\n",
    "    \n",
    "    def test_video(self, video_path, threshold=0.5, frame_interval=5):\n",
    "        \"\"\"Test video with memory-efficient frame processing\"\"\"\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Error: Could not open video file {video_path}\")\n",
    "                return False, 0.0\n",
    "                \n",
    "            frame_results = []\n",
    "            frame_count = 0\n",
    "            processed_count = 0\n",
    "            \n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                    \n",
    "                if frame_count % frame_interval == 0:  # Process every nth frame\n",
    "                    processed_frame = self.process_frame(frame)\n",
    "                    processed_frame = np.expand_dims(processed_frame, axis=0)\n",
    "                    detection_prob = self.detector.discriminator.predict(processed_frame, verbose=0)\n",
    "                    frame_results.append(detection_prob[0])\n",
    "                    processed_count += 1\n",
    "                \n",
    "                frame_count += 1\n",
    "                \n",
    "            cap.release()\n",
    "            \n",
    "            if not frame_results:\n",
    "                print(\"No frames processed in video\")\n",
    "                return False, 0.0\n",
    "            \n",
    "            # Visualization of frame-by-frame predictions\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.plot(frame_results)\n",
    "            plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "            plt.title('Frame-by-Frame Prediction Scores')\n",
    "            plt.xlabel('Processed Frame')\n",
    "            plt.ylabel('Deepfake Score')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.savefig('frame_predictions.png')\n",
    "            plt.close()\n",
    "            \n",
    "            avg_prob = np.mean(frame_results)\n",
    "            is_fake = avg_prob > threshold\n",
    "            confidence = avg_prob if is_fake else 1 - avg_prob\n",
    "            \n",
    "            print(\"\\nMemory-Efficient Test Results:\")\n",
    "            print(f\"Frames processed: {processed_count}/{frame_count}\")\n",
    "            print(f\"Average score: {avg_prob:.4f}\")\n",
    "            print(f\"Conclusion: {'FAKE' if is_fake else 'REAL'} (confidence: {confidence*100:.2f}%)\")\n",
    "            \n",
    "            return is_fake, confidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error while testing video: {str(e)}\")\n",
    "            return False, 0.0\n",
    "\n",
    "def main():\n",
    "    print(\"Building enhanced dataset...\")\n",
    "    dataset = MemoryEfficientDataset()\n",
    "    dataset.build_frame_dataset(max_samples_per_class=5000)  # Increased sample count\n",
    "    \n",
    "    # Get generators with augmentation\n",
    "    train_gen, train_steps, test_gen, test_steps = dataset.get_generator(BATCH_SIZE)\n",
    "    \n",
    "    # Initialize and train enhanced model\n",
    "    detector = EnhancedDeepfakeDetector()\n",
    "    detector.compile_model()\n",
    "    \n",
    "    print(\"\\nTraining enhanced model...\")\n",
    "    detector.train(train_gen, train_steps, test_gen, test_steps, epochs=EPOCHS)\n",
    "    \n",
    "    # Test video\n",
    "    print(\"\\nTesting video with enhanced model...\")\n",
    "    tester = EfficientVideoTester(detector)\n",
    "    is_fake, confidence = tester.test_video(TEST_VIDEO)\n",
    "    \n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(f\"Video '{TEST_VIDEO}' is classified as: {'FAKE' if is_fake else 'REAL'}\")\n",
    "    print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clear session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93fcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\araut1\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\araut1\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building full-frame dataset with all frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real videos: 100%|██████████| 166/166 [02:56<00:00,  1.06s/it]\n",
      "Processing fake videos:   0%|          | 0/166 [00:00<?, ?it/s]\n",
      "Processing fake videos: 100%|██████████| 166/166 [03:47<00:00,  1.37s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 444\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# Clear session to free memory\u001b[39;00m\n\u001b[32m    443\u001b[39m     tf.keras.backend.clear_session()\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 426\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    423\u001b[39m train_gen, train_steps, test_gen, test_steps = dataset.get_generator(BATCH_SIZE)\n\u001b[32m    425\u001b[39m \u001b[38;5;66;03m# Initialize and train model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m detector = \u001b[43mEnhancedDeepfakeDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m detector.compile_models()\n\u001b[32m    429\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining enhanced full-frame model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mEnhancedDeepfakeDetector.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28mself\u001b[39m.discriminator = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_enhanced_discriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mself\u001b[39m.generator = \u001b[38;5;28mself\u001b[39m.build_enhanced_generator()\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mself\u001b[39m.gan = \u001b[38;5;28mself\u001b[39m.build_enhanced_gan()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mEnhancedDeepfakeDetector.build_enhanced_discriminator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m x = layers.LeakyReLU(alpha=\u001b[32m0.2\u001b[39m)(x)\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Attention mechanism for better feature focus\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# PatchGAN output with higher resolution patches\u001b[39;00m\n\u001b[32m    192\u001b[39m patch_output = layers.Conv2D(\u001b[32m1\u001b[39m, (\u001b[32m4\u001b[39m,\u001b[32m4\u001b[39m), padding=\u001b[33m'\u001b[39m\u001b[33msame\u001b[39m\u001b[33m'\u001b[39m)(x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 209\u001b[39m, in \u001b[36mEnhancedDeepfakeDetector._attention_block\u001b[39m\u001b[34m(self, x, filters)\u001b[39m\n\u001b[32m    206\u001b[39m h = layers.Conv2D(filters, \u001b[32m1\u001b[39m)(x)\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Reshape for matrix multiplication\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m s = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtranspose_b\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    213\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m beta = tf.nn.softmax(s)\n\u001b[32m    216\u001b[39m o = tf.matmul(beta, layers.Reshape((-\u001b[32m1\u001b[39m, filters))(h))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[39m, in \u001b[36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    141\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m   bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m   bound_arguments.apply_defaults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:156\u001b[39m, in \u001b[36mKerasTensor.__tf_tensor__\u001b[39m\u001b[34m(self, dtype, name)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    159\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mused when constructing Keras Functional models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    160\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand `keras.ops`). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    165\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    169\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    171\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    172\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    173\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    175\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    176\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, mixed_precision\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VideoTester:\n",
    "    def __init__(self, model_path='saved_models/discriminator.h5'):\n",
    "        # Load the trained model\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.img_size = (500, 942)  # Should match your training size\n",
    "        \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame for prediction\"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (self.img_size[1], self.img_size[0]))\n",
    "        frame = (frame / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "        return np.expand_dims(frame, axis=0)\n",
    "    \n",
    "    def test_video(self, video_path, threshold=0.5, frame_interval=5):\n",
    "        \"\"\"Test a video file and return prediction results\"\"\"\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Error: Could not open video file {video_path}\")\n",
    "                return None\n",
    "            \n",
    "            frame_results = []\n",
    "            frame_count = 0\n",
    "            processed_count = 0\n",
    "            \n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                    \n",
    "                if frame_count % frame_interval == 0:  # Process every nth frame\n",
    "                    processed_frame = self.process_frame(frame)\n",
    "                    prediction = self.model.predict(processed_frame, verbose=0)[0][0]\n",
    "                    frame_results.append(prediction)\n",
    "                    processed_count += 1\n",
    "                \n",
    "                frame_count += 1\n",
    "                \n",
    "            cap.release()\n",
    "            \n",
    "            if not frame_results:\n",
    "                print(f\"No frames processed in video {video_path}\")\n",
    "                return None\n",
    "            \n",
    "            avg_prob = np.mean(frame_results)\n",
    "            is_fake = avg_prob > threshold\n",
    "            confidence = avg_prob if is_fake else 1 - avg_prob\n",
    "            \n",
    "            return {\n",
    "                'video_path': video_path,\n",
    "                'is_fake': is_fake,\n",
    "                'confidence': confidence,\n",
    "                'avg_score': avg_prob,\n",
    "                'frames_processed': f\"{processed_count}/{frame_count}\",\n",
    "                'frame_predictions': frame_results\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error while testing video {video_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def test_videos_in_folder(self, folder_path, threshold=0.5, frame_interval=5):\n",
    "        \"\"\"Test all videos in a folder and return results\"\"\"\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Error: Folder {folder_path} does not exist\")\n",
    "            return []\n",
    "            \n",
    "        video_files = [f for f in os.listdir(folder_path) \n",
    "                      if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "        \n",
    "        if not video_files:\n",
    "            print(f\"No video files found in {folder_path}\")\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        for video_file in tqdm(video_files, desc=\"Testing videos\"):\n",
    "            video_path = os.path.join(folder_path, video_file)\n",
    "            result = self.test_video(video_path, threshold, frame_interval)\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_report(self, results, output_file='test_results.txt'):\n",
    "        \"\"\"Generate a text report of the testing results\"\"\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"Deepfake Video Test Results\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            \n",
    "            for result in results:\n",
    "                f.write(f\"Video: {result['video_path']}\\n\")\n",
    "                f.write(f\"Classification: {'FAKE' if result['is_fake'] else 'REAL'}\\n\")\n",
    "                f.write(f\"Confidence: {result['confidence']*100:.2f}%\\n\")\n",
    "                f.write(f\"Average Score: {result['avg_score']:.4f}\\n\")\n",
    "                f.write(f\"Frames Processed: {result['frames_processed']}\\n\")\n",
    "                \n",
    "                # Plot frame predictions\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                plt.plot(result['frame_predictions'])\n",
    "                plt.axhline(y=0.5, color='r', linestyle='--')\n",
    "                plt.title(f\"Frame Predictions - {os.path.basename(result['video_path'])}\")\n",
    "                plt.xlabel('Processed Frame')\n",
    "                plt.ylabel('Deepfake Score')\n",
    "                plt.ylim(0, 1)\n",
    "                \n",
    "                plot_filename = f\"predictions_{os.path.splitext(os.path.basename(result['video_path']))[0]}.png\"\n",
    "                plt.savefig(plot_filename)\n",
    "                plt.close()\n",
    "                \n",
    "                f.write(f\"Prediction plot saved to: {plot_filename}\\n\")\n",
    "                f.write(\"\\n\" + \"-\"*50 + \"\\n\\n\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            if results:\n",
    "                fake_count = sum(1 for r in results if r['is_fake'])\n",
    "                real_count = len(results) - fake_count\n",
    "                avg_confidence = np.mean([r['confidence'] for r in results]) * 100\n",
    "                \n",
    "                f.write(\"\\nSummary Statistics:\\n\")\n",
    "                f.write(f\"Total Videos Tested: {len(results)}\\n\")\n",
    "                f.write(f\"Fake Videos Detected: {fake_count}\\n\")\n",
    "                f.write(f\"Real Videos Detected: {real_count}\\n\")\n",
    "                f.write(f\"Average Confidence: {avg_confidence:.2f}%\\n\")\n",
    "        \n",
    "        print(f\"\\nReport generated and saved to {output_file}\")\n",
    "\n",
    "def test_all_videos():\n",
    "    # Initialize the tester\n",
    "    tester = VideoTester()\n",
    "    \n",
    "    # Path to your test videos folder\n",
    "    test_folder = 'test_videos'\n",
    "    \n",
    "    # Test all videos in the folder\n",
    "    results = tester.test_videos_in_folder(test_folder)\n",
    "    \n",
    "    # Generate a report\n",
    "    if results:\n",
    "        tester.generate_report(results)\n",
    "    else:\n",
    "        print(\"No valid results to report.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_all_videos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
